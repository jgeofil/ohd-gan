\newglossaryentry{t-sne}{
    type=\acronymtype,
    name={t-SNE}, 
    description={The t-Distributed Stochastic Neighbor Embedding clustering algorithm is a nonlinear dimensionality reduction technique commonly applied to high-dimensional data. See \citet{maaten2008tsne}.}
    text={t-SNE},
    first={t-Distributed Stochastic Neighbor Embedding (t-SNE)}
}

\newglossaryentry{ward2icu}{
    name={Ward2ICU},
    description={Labelled synthetic dataset of vital signs indicating patient transitions from the ward to intensive care units generated by \citeauthor{severo2019ward2icu} using a \gls{cwgan-gp}.}
}

\newglossaryentry{mode-collapse}{
    type=\acronymtype,
    name={mode collapse}, 
    description={Mode collapse occurs when the \gls{gan} training procedure fails to converge, or converges to an undesirable local minima resulting in a lack of variety in the generated samples.}
    text={mode collapse},
    first={mode collapse}
}

\newglossaryentry{mb-avg}{
    type=\acronymtype,
    name={MB-Avg}, 
    description={Alternative to mini-batch discrimination that performs well on categorical features to cope with mode collapse, see \cite{Choi2017-nt}}
    text={MB-Avg},
    first={Mini-batch Averaging (MB-Avg)}
}

\newglossaryentry{dom-tran}{
    type=\acronymtype,
    name={DT}, 
    description={Applying a function to data points transforming them from one domain or category to another.}
    text={DT},
    first={Domain Translation (DT)}
}


\newglossaryentry{semi-sup}{
    type=\acronymtype,
    name={SSL}, 
    description={Semi-supervised learning refers to a type of \gls{ml} algorithm training procedure. Where in supervised learning all the data points are labelled and the algorithm is trained conditionally, and in unsupervised learning the data is unlabelled leaving the algorithm to discover patterns in the data, in semi-supervised learning only a small portion of the data is labelled. There is multitude of gls{ssl} algorithms and application, which generally involve learning from the labelled data points to gather information from the unlabelled. \cite{ssl-mit} }
    text={SSL},
    first={Semi-supervised learning (SSL)}
}
\newglossaryentry{re-iden}{
    type=\acronymtype,
    name={reidentification attack}, 
    description={See \gls{mi}}
    text={reidentification attack},
    first={reidentification attack}
}
\newglossaryentry{dbio}{
    type=\acronymtype,
    name={digital bio-markers}, 
    description={As opposed to classical bio-markers, which are broadly defined as any chemical, physical or biological indication of a patient's state the can be measured and are reproducible \cite{biomarkers2010}. Digital bio-markers are a trend emerging from the ubiquity of personal electronics devices which are said to have great potential as equivalent indicators, described as "[...] objective, quantifiable, physiological, and behavioural measures that are collected by sensors embedded in portable, wearable, implantable, or digestible devices." \cite{digital2020}
    text={digital bio-markers},
    first={digital bio-markers}
}}

\newglossaryentry{exploding}{
    type=\acronymtype,
    name={exploding gradients}, 
    description={When training a GAN, the gradients can accumulate large amounts of error, destabilising or disabling the training procedure.}
    text={exploding gradients},
    first={exploding gradients}
}

\newglossaryentry{vanishing}{
    type=\acronymtype,
    name={vanishing gradients}, 
    description={When training a GAN, the gradients become null and the network can no longer be updated.}
    text={vanishing gradients},
    first={vanishing gradients}
}

\newglossaryentry{a-disclosure}{
    type=\acronymtype,
    name={AD}, 
    description={Occures when an attacker can uncover confidential attributes of and individual from those released in an anonymized form.}
    text={AD},
    first={Attribute disclosure}
}

\newglossaryentry{mar}{
    type=\acronymtype,
    name={MaR}, 
    description={Given a dataset with missing entries , the missingness depends only on the observed variables \cite{yoon2018imputation}.}
    text={MaR},
    first={Missing at Random (MaR)}
}

\newglossaryentry{mcar}{
    type=\acronymtype,
    name={MCaR}, 
    description={Missing Completely at Random, description={Given a dataset with missing entries, the missingness is not dependant on any of the variables, thus occurs completely at random \cite{yoon2018imputation}.}}
    text={MCaR},
    first={Missing Completly at Random (MCaR)}
}

\newglossaryentry{}{
    type=\acronymtype,
    name={}, 
    description={}
    text={},
    first={}
}

\newglossaryentry{msn}{
 type=\acronymtype,
 name={MSN},
 description={Per feature, a variational Gaussian mixture model is used to estimate the number of modes and fit a Gaussian mixture. A one-hot vector indicating the mode, and a scalar indicating the value within the mode is produced. See \cite{Xu2019-ay}.},
 text={MSN},
 first={Mode-specific normalization (MSN)}
}

\newglossaryentry{tbs}{
    type=\acronymtype,
    name={TbS}, 
    description={To deal with the imbalance of values in categorical featues, during training the data is resampled in a way that all the categories from discrete attributes are sampled evenly, without inducing bias and so as to recover real data distribution. See \cite{Xu2019-ay} for a step-by-step spefication.}
    text={TbS},
    first={Training by sampling (TbS)}  
}

\newglossaryentry{nnaa}{
    type=\acronymtype,
    name={NN-AA}, 
    description={"Compares the distance from one point in a target distribution T, to the nearest point in a source distribution S, to the distance to the next nearest point in the target distribution." See \cite{yale2019ESANN}.}
    text={NN-AA},
    first={Nearest-neighbor Adversarial Accuracy (NN-AA)}
}

\newglossaryentry{pl}{
    type=\acronymtype,
    name={PL}, 
    description={Difference of \gls{nnaa} on the test set and on the training set. See \cite{yale2019ESANN}.}
    text={PL},
    first={Privacy loss (PL)}
}

\newglossaryentry{dt}{
    type=\acronymtype,
    name={DT}, 
    description={The discriminator is tested on batches of synthetic data produced by other methods to asses the possibility of over-fitting, see \cite{yale2019ESANN}.}
    text={DT},
    first={Discriminator testing (DT)}
}

\newglossaryentry{eicu}{
    type=\acronymtype,
    name={eICU}, 
    description={}
    text={eICU},
    first={Telehealth for the Intensive Care Unit (eICU)}
}

\newglossaryentry{do}{
    type=\acronymtype,
    name={DO}, 
    description={Privacy preservation method. See \cite{yale2019ESANN} based on \cite{Dwork2008, Prasser2017}.}
    text={DO},
    first={Data obfuscation (DO}
}

\newglossaryentry{pate}{
    type=\acronymtype,
    name={PATE}, 
    description={Differential privacy method, best described by \citeauthor{Papernot2017}: "The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as "teachers" for a "student" model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student's privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student's training) and formally, in terms of differential privacy." \cite{Papernot2017,Papernot2018}}
    text={PATE},
    first={Private Aggregation of Teacher Ensembles (PATE)}
}

\newglossaryentry{mbd}{
    type=\acronymtype,
    name={MBD}, 
    description={Training technique. See \cite{Salimans2016}}
    text={MBD},
    first={Mini-batch discrimination (MDB)}
}

\newglossaryentry{t-gan}{
    type=\acronymtype,
    name={T-GAN}, 
    description={Training technique to stabilise training. Allows the introduction of real sample information into the process of training the the generator. See \cite{Jolicoeur-Martineau2019, Su2018}}
    text={T-GAN},
    first={Turing \gls{gan}}
}

\newglossaryentry{corrnn}{
    type=\acronymtype,
    name={CorrNN}, 
    description={Learns a common representation of two views, taking into account their correlation. See \cite{Jolicoeur-Martineau2019, Su2018}}
    text={CorrNN},
    first={Correlation \gls{nn}}
}

\newglossaryentry{fullbba}{
    type=\acronymtype,
    name={FullBB}, 
    description={A \gls{mi} attack setting where an attacker has now knowledge of the internal workings of the generator, but can only sample from it.}
    text={FullBB},
    first={Full Black-box Attack}
}

\newglossaryentry{partbba}{
    type=\acronymtype,
    name={PartBB}, 
    description={Similar to to the \gls{fullbba} setting with the attacker having the additional knowledge about the latent input $z$.}
    text={PartBB},
    first={Partial Black-box Attack}
}

\newglossaryentry{wba}{
    type=\acronymtype,
    name={WBA},
    description={Similar to the \gls{partbba} and \gls{fullbba} settings, but with the attacker having full knowledge of the generator internals, including gradient information.},
    text={WBA},
    first={White-box Attack}
}

\newglossaryentry{bgan}{
    type=\acronymtype,
    name={BGAN},
    description={Method for training GANs with discrete data that uses the estimated difference measure from the discriminator to compute importance weights for generated samples, enabling back-propagation. Tends to push the generated samples to lie on the decision boundary of the discriminator, which also improves stability of training on continuous data \cite{hjelm2017boundaryseeking}},
    text={BGAN},
    first={Boundary-seeking \gls{gan}}
}

\newglossaryentry{sdv}{
    type=\acronymtype,
    name={SDV},
    description={Generative model for relational database based on Gaussian Copulas \cite{Patki_2016}. One of the few publications treating multi-relation tables in their original form (to out knowledge the only), and has attracted a fair readership. See \href{https://github.com/sdv-dev/SDV}{Github sdv-dev/SDV}.},
    text={SDV},
    first={Synthetic Data Vault}
}

\newglossaryentry{pm}{
    type=\acronymtype,
    name={PM}, 
    description={According to the NIH, Precision Medicine or "Personalized medicine is an emerging practice of medicine that uses an individual's genetic profile to guide decisions made in regard to the prevention, diagnosis, and treatment of disease. Knowledge of a patient's genetic profile can help doctors select the proper medication or therapy and administer it using the proper dose or regimen." \cite{Ackerman2009}}
    text={PM},
    first={Personalized Medicine}
}

\newglossaryentry{st}{
    type=\acronymtype,
    name={ST}, 
    description={The self-training and co-training methods use classifiers first trained on the portion of labelled data to predict the labels of unlabelled instances. The newly labelled samples with the highest confidence are added to the labelled set to retrain the classifiers. The process is repeated iteratively. In the words of \citeauthor{yu2019rare}, \textit{"[...] a classifier is initially trained on the small labeled set, and the trained classifier is used to classify the unlabeled set, which is assigned with pseudo labels. After that, the part of unlabeled set with the most confident pseudo labels are selected, and added into the labeled set. The classifier iteratively trains itself with the labeled data and selected unlabeled data."} \cite{yu2019rare}.}
    text={ST},
    first={Self-training (ST)}
}

\newglossaryentry{ct}{
    type=\acronymtype,
    name={CT}, 
    description={The self-training and co-training methods use classifiers first trained on the portion of labelled data to predict the labels of unlabelled instances. The newly labelled samples with the highest confidence are added to the labelled set to retrain the classifiers. The process is repeated iteratively. In the words of \cite{yu2019rare}, \textit{"[...] co-training splits the features of labeled set into two sub-sets as two views, which are conditionally independent. Two classifiers are trained on two sub-sets respectively, and classify the unlabeled set with pseudo labeled. Then, the most confident unlabeled data determined by one classifier is fed into another classifier as additional pseudo labeled data for further training." \cite{yu2019rare}.}
    text={CT},
    first={Co-training (CT)}
}}

\newglossaryentry{mi}{
    type=\acronymtype,
    name={MI}, 
    description={See \gls{pd}.}
    text={Membership Inference},
    first={Membership Inference (MI)}
}

\newglossaryentry{pda}{
    type=\acronymtype,
    name={PD}, 
    description={Broadly, a Membership Inference attack aims to determine if a particular record was used to train a machine learning model \cite{chen2019ganleaks}. There is no canonical process by which an attack is conducted, nor specification of the data assets initially in possession of the attacker. Attacks range from completely \gls{fullbba} where the attacker can only query data from the model, to \gls{wba} where the model and its parameters all fully exposed. For a comprehensive taxonomy of MIA, refer to \citeauthor{chen2019ganleaks, Jayaraman2019}.}
    text={PD},
    first={Presence Disclosure (PD)}
}

\newglossaryentry{dle}{
    type=\acronymtype,
    name={DLE}, 
    description={Drug Laboratory Effects refer the changes that a patient's medication can induce on medical laboratory analyses such as diagnostic tests, leading to misinterpretations and errors \cite{VanBalveren2018}. Merely keeping track of the large quantity of known interactions is still problematic and the number of possible combination is immense. Moreover the effects vary according to each patient physiology. \citeauthor{yahi2017generative} made use of GANs to predict these effect on an personalized basis \cite{yahi2017generative}. See also \gls{ite}.} 
    text={DLE},
    first={Drug Laboratory Effects (DLE)}
}

\newglossaryentry{ite}{
    type=\acronymtype,
    name={ITE}, 
    description={Given a patient and what we know about the person's medical history and state, and the probability of various possible outcomes of disease progression. The aim of Individualized Treatment effects is to estimate the consequences of administering a particular treatment and their likely-hood. The task is made particularly complex due the fact that for any given individual, the decision can only be made once. In other words, paired samples are lacking, making it impossible to compare the outcomes directly \cite{Haupt2019}.}
    text={ITE},
    first={Individualized Treatment effects (ITE)}
}

\newglossaryentry{mimic}{
    type=\acronymtype,
    name={MIMIC}, 
    description={Openly available dataset of deidentified health data associated with ~60,000 intensive care unit admissions. It includes demographics, vital signs, laboratory tests, medications, a range of data types from physiological time-series to free-text interpretation of radiology imaging \cite{johnson2016mimic}}
    text={MIMIC},
    first={Medical Information Mart for Intensive Care (MIMIC)}
}

\newacronym[type=oalgo]{medgan}{medGAN}{medGAN}
\newacronym[type=oalgo]{ssl-gan}{SSL-GAN}{Semi-supervised Learning with a learned ehrGAN}
\newacronym[type=oalgo]{wgantpp}{PPWGAN}{\gls{wgan} for Temporal Point-processes}
\newacronym[type=oalgo]{radialgan}{RadialGAN}{RadialGAN}
\newacronym[type=oalgo]{mc-arae}{MC-ARAE}{Multi-categorical \gls{arae}}
\newacronym[type=oalgo]{ctgan}{CTGAN}{Conditional Tabular \Gls{gan}}
\newacronym[type=oalgo]{heterogan}{HGAN}{Heterogeneous GAN}
\newacronym[type=oalgo]{emr-wgan}{EMR-WGAN}{EMR Wassertein GAN}
\newacronym[type=oalgo]{corgan}{corGAN}{corGAN}
\newacronym[type=oalgo]{1d-cae}{1D-CAE}{1-dimensional Convolutional \gls{ae}}
\newacronym[type=oalgo]{ehrgan}{ehrGAN}{Electronic Health Record GAN}
\newacronym[type=oalgo]{rgan}{RGAN}{Recurrent \gls{gan}}
\newacronym[type=oalgo]{rcgan}{RC-GAN}{Recurrent Convolutional \gls{gan}}
\newacronym[type=oalgo]{ganite}{GANITE}{Generative Adversarial Nets for inference of Individualized Treatment Effects}
\newacronym[type=oalgo]{cwr-gan}{CWR-GAN}{Cycle Wasserstein Regression \gls{gan}}
\newacronym[type=oalgo]{gain}{GAIN}{Generative Adversarial Imputation Network}
\newacronym[type=oalgo]{cgain}{CGAIN}{Categorical \gls{gain}}
\newacronym[type=oalgo]{mc-medgan}{MC-medGAN}{Multi-categorical \gls{medgan}}
\newacronym[type=oalgo]{mc-gumbelgan}{MC-GumbelGAN}{Multi-categorical Gumbel-softmax \gls{gan}}
\newacronym[type=oalgo]{mc-wgan-gp}{MC-WGAN-GP}{Multi-categorical \gls{wgan} with Gradient Penalty}
\newacronym[type=oalgo]{medbgan}{MedBGAN}{Boundary-seeking \gls{medgan}}
\newacronym[type=oalgo]{healthgan}{HealthGAN}{HealthGAN}
\newacronym[type=oalgo]{medwgan}{MedWGAN}{Wassertein \gls{medgan}}
\newacronym[type=oalgo]{sc-gan}{SC-GAN}{Sequentially Coupled \gls{gan}}
\newacronym[type=oalgo]{rmb}{RMB}{Restricted Boltzmann Machine}
\newacronym[type=oalgo]{anomigan}{AnomiGAN}{GANs for anonymizing private medical data}
\newacronym[type=oalgo]{wgan-gp}{WGAN-GP}{\gls{wgan} with Gradient Penalty}
\newacronym[type=oalgo]{dp-auto-gan}{DP-auto-GAN}{\gls{dp}-auto-\gls{gan}}
\newacronym[type=oalgo]{ads-gan}{ADS-GAN}{Anonymization through data synthesis using \gls{gan}}
\newacronym[type=oalgo]{gcgan}{GcGAN}{\gls{corrnn} and \gls{t-wgan}}
\newacronym[type=oalgo]{t-wgan}{T-wGAN}{Wassertein \gls{t-gan}}
\newacronym[type=oalgo]{conan}{CONAN}{\textit{Co}plementary patter\textbf{n A}augmentatio\textbf{n}}
\newacronym[type=oalgo]{cwgan-gp}{cWGAN-GP}{Conditional \gls{wgan-gp}}
\newacronym[type=oalgo]{pate-gan}{PATE-GAN}{Private Aggregation of Teacher Ensembles (PATE) framework applied to GANs}
\newacronym[type=oalgo]{ac-gan}{AC-GAN}{Auxiliary Classifier \gls{gan}}
\newacronym[type=oalgo]{glugan}{GluGAN}{Blood Glucose \gls{gan}}
\newacronym[type=oalgo]{wgan-dp}{WGAN-DP}{\gls{wgan} with \gls{dp}}
\newacronym[type=oalgo]{rsdgm}{RSDGM}{Realistic Synthetic Dataset Generation Method}



%\newacronym{}{}{}
\newacronym{nn}{NN}{Neural Network}
\newacronym{gan}{GAN}{Generative Adversarial Network}
\newacronym{ffn}{FFN}{Feed-forward Network}
\newacronym{ae}{AE}{Autoencoder}
\newacronym{rnn}{RNN}{Recurrent \gls{nn}}
\newacronym{lstm}{LSTM}{Long Short-term Memory}
\newacronym{cgan}{CGAN}{Conditional \gls{gan}}
\newacronym{crmb}{CRMB}{Conditional Restricted Boltzmann Machine}
\newacronym{cnn}{CNN}{Convolutional \gls{nn}}
\newacronym{wgan}{WGAN}{Wassertein \gls{gan}}
\newglossaryentry{beta-vae}{
    name = {0xCE-VAE},
    description = {0xCE variational auto-encoder}
}
\newacronym{lr}{LR}{Logistic-regression}
\newacronym{cycle-gan}{Cycle-GAN}{Cycle-consistent \gls{gan}}
\newacronym{adtep}{ADTEP}{Adversarial Deep Treatment Effect Prediction}
\newacronym{cae}{CAE}{Convolutional \gls{ae}}
\newacronym{gru}{GRU}{Gated Recurrent Unit}
\newacronym{gumbel-gan}{Gumbel-GAN}{Gumbel-Softmax \gls{gan}}
\newacronym{arae}{ARAE}{Adversarially regularized autoencoder}



%% Terms
\newacronym{ml}{ML}{Machine Learning}
\newacronym{ohd-gan}{OHD-GAN}{\glspl{gan} for Observation Health Data}
\newacronym{sd}{SD}{Synthetic Data}
\newacronym{ohd}{OHD}{Observational Health Data}
\newacronym{ehr}{EHR}{Electronic Health Record}
\newacronym{icu}{ICU}{Intensive Care Unit}
\newacronym{pmf}{PMF}{Probability Mass Function}
\newacronym{ssl}{SSL}{Semi-supervised learning}
\newacronym{cqm}{CQM}{Clinical Quality Measure}
\newacronym{hi}{HI}{Health Informatics}
\newacronym{pd}{PD}{Probability Distribution}
\newacronym{ks}{KS}{Kolmogorov-Smirnov}


\newacronym{iot}{IoT}{Internet of Things}

%% Techniques
\newacronym{bn}{BN}{batch-normalization}
\newacronym{sc}{SC}{shortcut connections}
\newacronym{cbt}{CBT}{Cluster-based training}
\newacronym{vcd}{VCD}{Variational contrastive divergence}
\newacronym{ln}{LN}{Layer normalisation}
\newacronym{sn}{SN}{Spectral Normalization}
\newacronym{dsm}{DSM}{Domain Specific Measure}
\newacronym{dwp}{DWP}{Dimension-wise prediction}
\newacronym{arm}{ARM}{Association Rule Mining}
\newacronym{trts}{TRTS}{Train on synthetic, test on real}
\newacronym{tstr}{TSTR}{Train on real, test on synthetic}
%% Privacy
\newacronym{dp}{DP}{Differential privacy}
\newacronym{dp-sgd}{DP-SGD}{Differential private stochastic gradient descent}
\newacronym{ad}{AD}{Attribute Disclosure}
\newacronym{rr}{RR}{Reproduction rate}

\newacronym{anm}{ANM}{Additive noise model}
\newacronym{vae}{VAE}{Variational AE}
%% Metrics
\newacronym{fop}{F-OP}{First-order proximity}
\newacronym{cc}{CC}{Correlation coefficient}
\newacronym{mmd}{MMD}{Maximum Mean Discrepency}
\newacronym{rbf}{RBF}{Radial Basis Function}
\newacronym{mse}{MSE}{Mean Squared Error}
\newacronym{auroc}{AUROC}{Area under ROC curve}
\newacronym{auprc}{AUPRC}{Area under the precision-recall curve}
\newacronym{kld}{KLD}{Kullback-Leibler divergence}
\newacronym{fd}{FD}{Feature distributions}
\newacronym{qq}{QQ}{Quantile-quantile plot}
\newacronym{lsr}{LSR}{Latent space representation}
\newacronym{rdp}{RDP}{Renyi Differential Privacy}
\newacronym{pcam}{PCAM}{\gls{pca} Marginal}
\newacronym{pca}{PCA}{Principal Component Analysis}
\newacronym{pcawdd}{PCA-DWD}{\gls{pca} Distributoinal Wassertein Distance}
\newacronym{pta}{PTA}{Prediction task accuracy}
\newacronym{ssa}{SSA}{Semi-supervised augmentation}
\newacronym{dwpro}{DWS}{Dimension-wise Statistics}
\newacronym{dwpre}{DWP}{Dimension-wise Prediction}
\newacronym{ved}{VED}{Visual Expert Discrimination}
\newacronym{rf}{RF}{Random Forest}
\newacronym{svm}{SVM}{Support Vector Machine}
\newacronym{rmse}{RMSE}{Root Mean-Squared Rrror}
\newacronym{rvts}{RV-TS}{real-valued time-series}
\newacronym{dts}{D-TS}{discrete time-series}
\newacronym{vs}{VS}{Variable Splitting}
\newacronym{it}{IT}{Iterative Imputation}
\newacronym{bp}{BP}{Backpropagation \gls{it}}
\newacronym{pr}{PR}{Posterior Regularization}
\newacronym{rl}{RL}{Reinforcement learning}
\newacronym{ps}{PS}{Posterior Regularization}
\newacronym{irl}{IRL}{Inverse \gls{rl}}
\newacronym{hexagan}{HexaGAN}{Six component \gls{gan}}
\newacronym{miwae}{MIWAE}{Missing data IWAE}
\newacronym{hi-vae}{HI-VAE}{Heterogeneous-Incomplete VAE}
\newacronym{mida}{MIDA}{Multiple Imputation Denoising Autoencoders}
\newacronym{info-gan}{InfoGAN}{Information Maximizing \gls{gan}}
\newacronym{eeg}{EEG}{Electroencephalogram}
\newacronym{ecg}{ECG}{Electrocardiogram}