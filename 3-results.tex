\section{Results}
    \subsection{Summary}
        We have found a total of 43 \todo{recount and check for more} publications describing the development or adaption of \gls{ohd-gan}, presented in Table \ref{tab:3:publications}. The type of data addressed in each of these publications can be generalized into one of two categories: time-dependent observations, such as time-series, or static representation in the form of feature vectors such as tabular rows.\par
        
        Most efforts propose adaptations of current algorithms to the characteristics and complexities of \gls{ohd}. These include multi-modality of marginal distributions or non-Gaussian real-valued features, heterogeneity, a combination of discrete and real-valued features, longitudinal irregularity, complex conditional distributions, missingness or sparsity, class imbalance of categorical features and noise.\par 
        
        While these properties may make training a useful model difficult, the variety of applications that are highly relevant and needed in the healthcare domain provide sufficient incentive. The most cited motives are, as one would expect, to cope with the often limited number of samples in medical datasets and to overcome the highly restricted access to \gls{ohd}. The potential of releasing privacy-preserving \gls{sd} freely is a common subject. Publications considering privacy evaluate the effect on utility of applying \gls{dp} to their algorithm, propose alternatives privacy concepts and metrics, or exclusively concentrate on the subject of privacy.\par
        
    \subsection{Motives for developing OHD-GAN}
        Some claim that the ability to generate synthetic is becoming an essential skill in data science \cite{Sarkar2018}, but what purpose can it serve in the medical domain? The authors mention a wide range of potential applications. We briefly describe the four prevailing themes in the following sections: data augmentation (Sec.\ref{sec:augmentation}), privacy and accessibility (Sec.\ref{sec:access_privacy}), precision medicine (Sec.\ref{sec:precision_med}) and  modelling simulations (Sec.\ref{sec:models_twins}). 

        \subsubsection{Data augmentation}\label{sec:augmentation}
    
            Data augmentation \todo{define} is mentioned in nearly all publications. Although counter-intuitive, it is well known that \gls{gan} can generate \gls{sd} that conveys more information about the real data distribution. Effectively, the real-valued space distribution of the generator produces a more comprehensive set of data points, valid, but not present in the discrete real data points. A combination of real and synthetic training data habitually leads to increased predictor performance \cite{Wang_2019,Che_2017,Yoon2018-ite, yoon2018imputation, Yang_2019_impute_ehr, Chen_2019, cui2019conan, Che_2017}. A more intelligible way to seize the concept from the point of view of image classification, in which it is known as invariances, perturbations such as rotation, shift, sheer and scale \cite{antoniou2017data}.\par 
            
            Similarly, domain translation \todo{define} and \gls{semi-sup} training approaches with \glspl{gan} could support predictive tasks that lack data with accurate labels, lack paired samples or suffer class imbalance \cite{Che_2017,mcdermott2018semi, Yoon2018-ite}. Another example is correcting discrepancies between datasets collected in different locations or under different conditions inducing bias \cite{Yoon2018-radial}. \glspl{gan} are also well adapted for data imputation, were  entries are \gls{mar} \cite{yoon2018imputation}. 

        \subsubsection{Enhancing privacy and increasing data accessibility}\label{sec:access_privacy}
    
            \gls{sd} is seen as the key to unlocking the unexploited value of \gls{ohd} hindering machine learning, and more generally scientific progress \cite{Beaulieu-Jones2019-ct, baowaly_2019_IEEE,baowaly_2019_jamia,Che_2017,esteban2017real,Fisher2019,severo2019ward2icu}. Preserving privacy can broadly be described as reducing the risk of \gls{re-iden} to an acceptable level. This level of risk is quantified when releasing data anonymized with \gls{dp}.\par
    
            Due to its artificial nature, \gls{sd} is put forward as a means to forgo the tight restrictions on data sharing, while potentially providing greater privacy guarantees \cite{Beaulieu-Jones2019-ct, baowaly_2019_IEEE, baowaly_2019_jamia,esteban2017real,Fisher2019,walsh2020generating, chin2019generation}. Enabling access to greater variety, quality and quantity of \gls{ohd} could have positive effects in a wide range of fields, such as software development, education, and training of medical professionals. The fact remains that \glspl{gan} do not eliminate the risk of reidentification. Considering none of the synthetic data points represent real people, the significance of such an occurrence is unclear. \todo{find some more information} Nonetheless, both methods can be combined, and \gls{gan} training according to \gls{dp} shows evidence of reducing the loss of utility in comparison to \gls{dp} alone. \todo{Find these citations} Overall,  
    
        \subsubsection{Enabling precision medicine}\label{sec:precision_med}
    
            The application to precision medicine generally involve predicting outcomes conditioned on a patient's current state and history. Simulated trajectories could help inform clinical decision making by quantifying disease progression and outcomes and have a transformative effect on healthcare \cite{walsh2020generating, Fisher2019}. Ensembles of stochastic simulations of individual patient profiles such as those produced by \gls{crmb} could help quantify risk at an unprecedented level of granularity \cite{Fisher2019}.\par
            Predicting patient-specific responses to drugs is still a new field of research, a problem known as \gls{ite}. The task of estimating \gls{ite} is persistently hampered by the lack of paired counterfactual samples \cite{Yoon2018-ite, chu2019treatment}. To solve similar problems n medical imaging, various \gls{gan} algorithms were developed for domain translation, mapping a sample from its to original class to the paired equivalent. This includes bidirectional transformations, allowing \gls{gan} to learn mappings from very few, or a lack of paired samples \cite{Wolterink2017DeepMT, CycleGAN2017, mcdermott2018semi}.
    
        \subsubsection{From patient and disease models to digital twins}\label{sec:models_twins}
    
            A well trained model approximates the process that generated the real data points. In other words, the relations learned by the model, its parameters, contains meaningful information if we can learn to harness it. Data-driven algorithms evolve as our understanding of their behavior improves. New concepts are incorporated in the algorithms leading to further understanding, iterativly blurring the line with theory-driven approaches \cite{Hand2019}. Interpretability is a growing field of research concerned with understanding how the learned parameters of a model relate. In other words analysing the representation the algorithm has converged to and deriving meaning from seemingly obscure logic.Incorporating new understanding in the architecture of algorithms shift the view from a data-driven to a theory-driven perspective \cite{Hand2019}. As we purposefully build structure in our algorithms from new understanding we may get the chance to explore meaningful representations that would otherwise be beyond our reasoning.\par 
            
            Approaching these ideas from above, the concept of "digital twins" represents in a way the ultimate realization of \gls{pm}. A common practice in industrial sectors is high-fidelity virtual representations of physical assets. Long-term simulations, that provide an overview and comprehensive understanding of the workings, behavior and life-cycle of their real counterparts. The state of the models is continuously updated from theoretical data, real data and streaming \gls{iot} indicators.\par
            Intently conditioned input data allows the exploration of specific events or conditions. In a position paper on the subject, Angulo et al. draw the parallels of this technique with the current needs in healthcare and the emergence of the necessary technologies for actionable models of patients. \cite{angulo2019towards,Angulo_2020}. The authors bring up the rapid adoption of wearables that are continuously monitoring people's physiological state. 
            Wearables are one of many mobile digitally connected devices that collect patient data over a broad range of physiological characteristic and behavioral patterns \cite{coravos2019developing}. This emerging trend known as \gls{dbio} has already led to studies demonstrating predictive models with the potential for improved patient care \cite{snyder2018best}. Through continuous lifelong learning, integrating  multiple modes of personal data, generative patient models could inform diagnostics of medical professionals and also enable testing treatment options. In their proposal, \gls{gan} are an essential component of the ecosystem to ensure patient privacy and to provide bootstrap data. Fisher et al. already employ the term "digital twin" to describe their process, noting that they present no privacy risk and enable simulating patient cohorts of any size and characteristics \cite{walsh2020generating}.
        
            \input{tables/publications}

    \subsection{Data Types and Feature Engineering}

        No publications made use of \gls{ohd} in its initial form, patient records in \gls{ehr} composed of many related tables (normalized form). The complexity of a model would grow rapidly when maintaining referential integrity and statistics between multiple tables. The hierarchy by witch these would interact with each other conditionally is no less complicated \cite{Buda2015, Patki_2016, Zhang2015, Tay2013}. There are however published \gls{gan} algorithms made to consume normalized database in their original form. \todo In regards to \gls{ohd}, feature engineering was used to adapt the data to task requirements, or to a promising algorithms that fit the date characteristics. The data is transformed into one of four modalities: time series, point-processes, ordered sequences or aggregates described in Fig. \ref{tab:features}.

        \input{tables/features}

    \subsection{Data oriented GAN development}\label{subsec:data_gan_dev}

        \subsubsection{Auto-encoders and categorical features}\label{subsubsec:categorical}

            In what is to the best of our knowledge, the first attempt at developing a \gls{gan} for OHD. \citeauthor{Choi2017-nt} focus on the problem posed by the incompatibility of categorical and ordinal features with back-propagation. Their solution is to pretrain an \gls{ae} to project the samples to and from a continuous latent space representation. The decoder portion is retained along with its trained weights to form a component of \gls{medgan} \cite{Choi2017-nt}. It is incorporated into the generator and maps the randomly sampled input vectors from the real-valued latent space representation back to discrete features. This first exemplar of synthetic OHD generated by \gls{gan} inspires a series of enhancements.\par

            Numerous efforts were made to improve the performance of \gls{medgan}. Among the first, \citeauthor{Camino2018-re} developed \gls{mc-medgan} in which they modified the \gls{ae} component by splitting its output into a Gumbel-Softmax \cite{jang2016categorical} activation layer for each categorical variable and concatenating the results. \cite{Camino2018-re}. The authors also developed an adaptation based on recent training techniques: \gls{wgan} \cite{arjovsky2017wasserstein} and a \gls{wgan} with Gradient Penalty \cite{gulrajani2017improved}. In brief, the Wasserstein distance is a measure between two \glspl{pd} that has the property of always providing a smooth gradient. When used as the loss function of the discriminator, it generally improves training stability and mitigates mode collapse. The Wassertein loss function a 1-Lipschitz constraint that was originally solved by weight clipping. It was however demonstrated that in some cases this prevented the network from modelling the optimal function, thus Gradient penalty, a less restrictive regularization was introduced \cite{Petzka2018}. \gls{mc-wgan-gp} is the equivalent of \gls{mc-medgan} but with Softmax layers. The authors report that the choice of a model will depend on data characteristics, particularly sparsity.\par
            
            Wasserstein's distance was widely adopted by subsequent authors owing to the propensity of OHD to induce mode collapse. Baowaly et al. developed \gls{medwgan} also based on \gls{wgan}, and \gls{medbgan} borrowing from Boundary-seeking \gls{gan} (BGAN) \cite{hjelm2017boundaryseeking} which pushes the generator to produce samples that lie on the decision boundary of the discriminator, expanding the search space. Both led to improved data quality, in particular \gls{medbgan} \cite{baowaly_2019_IEEE,baowaly_2019_jamia}. In other effort, \citeauthor{Jackson_2019} tested \gls{medgan} on an extended dataset containing demographic and health system usage information, obtaining results similar to the original \cite{Jackson_2019}. The \gls{healthgan} built upon \gls{wgan-gp}, but includes a data transformation method adapted from the Synthetic Data Vault \cite{Patki_2016} to map categorical features to and from the unit numerical range \cite{Yale_2020}. 
        
        \subsubsection{Forgoing the autoencoder and conditional training}\label{noauto}

            Claiming that the use of an \gls{ae} introduces noise, with \gls{emr-wgan}, \citeauthor{Zhang2020} dispose of the \gls{ae} component of previous algorithms and introduce a conditional training method, along with conditioned \gls{bn} and \gls{ln} techniques to stabilise training \cite{Zhang2020}. The algorithm was further adapted by \citeauthor{yan2020generating} as \gls{heterogan} to better account for the conditional distributions between multiple data types and enforce record-wise consistency. A recognized problem with \gls{medgan} was that it produced common-sense inconsistencies, such as gender mismatches in medical codes \cite{yan2020generating, Choi2017-nt}. In \gls{heterogan}, constraints are enforced by adding specific penalties to the loss function, such as limit ranges for numerical categorical pairs and mutual exclusivity for pairs of binary features \cite{yan2020generating}. The algorithm also performs well on regular time-series of sleep patterns \cite{dash2019synthetic} \par

            To develop \gls{ctgan}, \citeauthor{Xu2019-ay} presume that tabular data poses a challenge to \gls{gan} owing to the non-Gaussian multi-modal distribution of real-valued columns and imbalanced discrete columns \cite{Xu2019-ay}. Their algorithm, composed of fully connected layers, was developed with adaptations to deal with both real-valued and categorical features. For real-valued features, it employs \todo{define} mode-specific normalization  to capture the multiplicity of modes. For discrete features conditional \todo{define} training-by sampling is devised to re-sample discrete attributes evenly during training, while recovering the real distribution when generating data.\par
            
            In other efforts, \citeauthor{torfi2019generating} develop \gls{corgan}, in which the \gls{ae} is replaced by a \gls{1d-cae} to capture neighboring feature correlations of the input vectors \cite{torfi2019generating}. \citeauthor{chincheong2020generation} use a \gls{ffn} based on Wassertein distance to evaluate the capacity of \glspl{gan} to model heterogeneous data of dense and sparse medical features \cite{chincheong2020generation}. \citeauthor{ozyigit2020generation} use the same approach with regards to reproducing statistical properties \cite{ozyigit2020generation}.
            
        \subsubsection{Time-series}
            
             Reproducing physiological time-series \citeauthor{esteban2017real} devise the \gls{rgan} and \gls{rcgan} based on \gls{lstm} to generate a regular time-series of physiological measurements from bedside monitors \cite{esteban2017real}. Curiously, the authors dismiss Wassertein's distance, stating that they did not find application in their experiments. In addition, each dimension of their time-series is generated independently from the others, where one would assume they are correlated. A considerable loss of accuracy is observed on their utility metric. In a benchmark on non-health data with long-term dependencies and complex multidimensional relationships against DopelGANger it was outperformed \cite{Lin2019}. \todo{Move to discussion}

    \subsection{Task oriented GAN development}
        \subsubsection{Semi-supervised learning}

            To develop \gls{ehrgan}, an algorithm for sequences of medical codes that learns a transitional distribution, \citeauthor{Che_2017} combine an Encoder-Decoder \gls{cnn} \cite{Rankin2020} with \gls{vcd} \cite{Che_2017}. The \gls{ehrgan} generator is trained to decode a random vector mixed with the latent space representation of a real patient. The trained \gls{ehrgan} model is then incorporated into the loss function of a predictor where it can help generalization by producing neighbors for each input sample.\par
            
            \Gls{ssl} is commonly employed to augment the minority class in imbalanced datasets, techniques such as \gls{st} and \gls{ct}. \citeauthor{yang2018unpaired} improve on both of these by incorporating a \gls{gan} in the procedure \cite{yang2018unpaired}. The \gls{gan} is first trained on the labelled set and used to re-balance it. A prediction task with a classifier ensemble is then executed and the data points with highest prediction confidence are labelled. The process is iterated until labelling expansion ceases. As a final step, the \gls{gan} is trained on the expanded labelled set to generate an equal amount of augmentation data. The authors obtained improved performance in a number of classification tasks and multiple tabular datasets with their method.
    
    \subsubsection{Domain translation}
    
        To address the heterogeneity of healthcare data originating from different sources, \citeauthor{Yoon2018-radial} combines the concepts of cycle-consistent domain translation \todo{define} from \gls{cycle-gan} \cite{Zhu_2017} and  multi-domain translation from Star-GAN \cite{choi2017stargan} to build \gls{radialgan} to translate heterogeneous patient information from different hospitals, correcting features and distribution mismatches \cite{Yoon2018-radial}. The algorithm use an encoder-decoder pair per data endpoint that are trained to map records to and from a shared latent representation for their respective endpoint. 
    
    \subsubsection{Individualized treatment effects}
    
        The task of estimating \glspl{ite} is an ongoing problem. \glspl{ite} refer to the response of a patient to a certain treatment given a set of characterizing features. This is due to the fact that counterfactual outcomes are never observed or treatment selection is highly biased \cite{Yoon2018-ite, mcdermott2018semi, walsh2020generating}. To overcome this problem, in \gls{ganite} \citeauthor{Yoon2018-ite} employ a pair of \glspl{gan}, one for counterfactual imputation and another for \gls{ite} estimation \cite{Yoon2018-ite}. The former captures the uncertainty in unobserved outcomes by generating a variety of counterfactuals. The output is fed to the latter, which estimates treatment effects and provides confidence intervals.\par
    
        With \gls{cwr-gan}, a joint regression-adversarial model, \citeauthor{mcdermott2018semi} demonstrated a \gls{ssl} approach inspired by \gls{cycle-gan} to leverage large amounts of unpaired pre/post-treatment time-series in \gls{icu} data for the estimation of \glspl{ite} on physiological time-series \cite{mcdermott2018semi}. The algorithm has the ability to learn from unpaired samples, with very few paired samples, to reversibly translate the pre/post-treatment physiological series.\par 
    
        \citeauthor{chu2019treatment} approach the problem of data scarcity by designing \gls{adtep}, an algorithm that can maximize use of the large volume of \gls{ehr} data formed by triples of non-task specific patient features, treatment interventions and treatment outcomes \cite{chu2019treatment}. \gls{adtep} learns representation and discriminatory features of the patient, and treatment data by training an \gls{ae} for each pair of features. In addition to \gls{ae} reconstruction loss, a second model is tasked with advsersarially identifying fake treatment feature reconstructions. Finally, a fourth loss metric is calculated by feeding the concatenated latent representations of both \glspl{ae} to a \gls{lr} model aimed at predicting the treatment outcome \cite{chu2019treatment}.\par
    
        Similarly to \citeauthor{esteban2017real}, \citeauthor{Wang_2019} demonstrated an algorithm to generate a time series of patient states and medication dosages pairs using \gls{lstm}. In contrast to \gls{rgan} and \gls{rcgan}, in \gls{sc-gan}, patients state at the current time-step informs the concurrent medication dosage, which in turn affects the patient state in the upcoming time-step \cite{Wang_2019}. \gls{sc-gan} overcame a number of baselines on both statistical and utility metrics.
    
    \subsubsection{Data imputation and augmentation}
    
        \gls{gan} are naturally suited for data imputation, and could provide a new approach to deal with the problems of health data relating to widespread missingness. Statistical models developed for the multiple imputation problem increase quadratically in complexity with the number of features, while the expressiveness of deep neural networks can efficiently model all features with missing values simultaneously.In that regard, \citeauthor{yoon2018imputation} adapted the standard \gls{gan} to perform imputation on real-valued features \gls{mar} in tabular datasets \cite{yoon2018imputation}. In \gls{gain}, the discriminator is tasked with classifying individual variables as real or fake (imputed), as opposed to the whole ensemble. Additional input, or hint, containing the probability of each component being real or imputed is fed to the discriminator to resolve the multiplicity of optimal distributions that the generator could reproduce. The model performs considerably better than five state-of-the-art benchmarks. \gls{gain} was later adapted by \citeauthor{Yang_2019_impute_ehr} to also handle categorical features using fuzzy binary encoding, the same technique employed in \gls{healthgan}. In parallel, \citeauthor{Camino2019} apply the same \gls{vs} technique they used fir \gls{medgan} to adapt \gls{gain} and run a benchmark against different types of \gls{vae}.\par
    
        The distribution estimated by a generator model can compensate for lack of diversity in a real sample, essentially filling in the blanks in a manner comparable to data imputation. In such cases, data sampled from this distribution has the potential to help improve generalization in training predictive models. We find evidence of this in generating unobserved counterfactual outcomes \cite{yoon2018imputation}, or generating neighboring samples to help generalization in predictors \cite{Che_2017}. The adversarially trained \gls{rmb} developed by \citeauthor{Fisher2019} enabled them to simulate individualized patient trajectories based on their base state characteristics. Due to the stochastic nature of the algorithm, generating a large number of trajectories for a single patient can provide new insights on the influence of starting conditions on disease progression or quantify risk \cite{Fisher2019}.
        
    \subsection{Model validation and data evaluation}
    
        To asses the solution to a generative modelling problem, it is necessary to validate the model, and to verify its output. \gls{gan} aim to approximate a data distribution $P$, using a parameterized model distribution $Q$ \cite{Borji2018-fy}. Thus, in evaluating the model, the goal is to validate that the learning process has led to a sufficiently close approximation. What this means in practice is hard to define. The concept of "realism" finds more natural application to images of text, but is more ambiguous when faced with the complexity of health data. \citeauthor{walsh2020generating} employ the term "statistical indistinguishability" and define it as the inability of a classification algorithm to differentiate real from synthetic samples \cite{walsh2020generating}. The terms covers almost all evaluation methods employed in the publications, which can be divided into two broad categories: those aimed at evaluating the statistical properties of the data directly, and those aimed at doing so indirectly by quantifying the work that can be done with the data. There are, nonetheless a few attempts of a qualitative nature, more in line with the concept of realism. 

        \subsubsection{Qualitative evaluation}
        
            Visual inspection of projections of the \gls{sd} is a common theme, serving mostly as a basic sanity check, but occasionally presented as evidence. The formal qualitative evaluation approaches found in the literature are mainly Preference Judgement, Discrimination Tasks or Clinician Evaluation and are generally carried out by medical professionals in the appropriate field (Borji 2018).
                \begin{itemize}
                    \item \textbf{Preference judgment} The task is choosing the most realistic of two data points in pairs of one real and one synthetic \cite{Choi2017-nt}.
                    \item \textbf{Discrimination Tasks} Data points are shown one by one and must be classified as real or synthetic \cite{Beaulieu-Jones2019-ct}.
                    \item \textbf{Clinician Evaluation} Rather than classifying the data points, they must be rated for realism according to a predefined numerical scale. \cite{Beaulieu-Jones2019-ct}. Significance is determined with a statistical test such as \todo{define Mann-Whitney}Mann-Whitney.
                    \item \textbf{Visualized embeding} The real and synthetic data samples are plotted on a graph or projected into an embeding such as \gls{t-sne} or PCA and compared visually. \cite{cui2019conan, yu2019rare, zhu_2020, yale2019ESANN, Yang_2019_ehr,Beaulieu-Jones2019-ct, tanti2019, dash2019synthetic}.
                    \item \textbf{Feature analysis} In certain fields, the data can be projected to representations that highlight patterns or properties that can be easily visually assessed. While this does not provide conclusive evidence of data realism, it can help get a better understanding of model behaviour during training. As an example, typical and easily distinguishable patterns in EEG and ECG bio-signals. \cite{Harada2019}
                \end{itemize}
    
        In general, qualitative evaluation methods based on visual inspection are weak indicators of data quality. At the dataset or sample level, quantitative metrics provide more convincing evidence of data quality (Borji 2018). That being said, it is highly possible that visual representations that were part of the evaluation were unintelligible.
        
        \subsubsection{Quantitative evaluation}
        
            Quantitative evaluation metrics can be categorized into three loosely defined groups: those comparing the distributions of real and synthetic data as a whole, those aimed at assessing the marginal and conditional distributions of features, and those evaluating the quality of the data indirectly by quantifying the amount of work that can be done with the data, referred to as utility.
            
            \begin{itemize}
                \item \textbf{Dataset distributions}
                A summary of metrics based on comparing distributions is presented in Tab. \ref{tab:3:distributions}.
                \item \textbf{Feature Distributions}
                If the model has learned a realistic representation of the real data it should produce \gls{sd} that possesses the same quantity and type of information content. Authors attempt by various metrics to determine if the statistical properties of the \gls{sd} agree with those of the real data. These metrics are presented in Table \ref{tab:3:statistics}. Although statistical similarity provides strong support for the behavior of the learning process, it is not necessarily informative about their validity. They are often ambiguous and can be found to be misleading upon further investigation. Given the complexity of health data, low level relations are unlikely to paint a full picture. Authors often state that no single metric taken on its own was sufficient, and that a combination of them allowed deeper understanding of the data.
                \item \textbf{Data utility}
                 Utility-based metrics often provide a more convincing indicator of data realism, on the other hand they mostly lack the interpretability that some statistical metrics allow. These are presented in Table \ref{tab:3:augmentation}. We took the liberty of placing these into one of two categories: tasks mostly defined for evaluation (Ad hoc utility metrics) or tasks based on real-world applications (Application utility metrics). Note that this distinction is not based on a rigorous definition, but serves to facilitate understanding.
                \item \textbf{Analytical} The analytical methods were mainly employed for evaluation, but can also provide a better understanding of the and its behavior.
                \begin{itemize}
                    \item \textsl{Feature Importance} The important features (\gls{rf}) and model coefficients (\gls{lr}, \gls{svm}) of predictors trained for some task are compared between real and synthetic data. \cite{esteban2017real,Xu2019-ay,Yoon2020-anon,chin2019generation, Beaulieu-Jones2019-ct}.
                    \item \textsl{Ablation study}  The performance of the model is compared against versions impared version, with some components removed. This helps determining if the novel component of the algorithm contributes significantly to performance \cite{cui2019conan, Che_2017, mcdermott2018semi, Yoon2018-radial, chincheong2020generation}.
                \end{itemize}
            \end{itemize}
            
                \input{tables/distributions}
                \input{tables/statistics}
                \input{tables/augmentation}

    \subsection{Alternative evaluation}
        In their publications, \citeauthor{Yale_2020} propose refreshing approaches to evaluating the utility of \gls{sd}. For example, they organized a hack-a-thon type challenge involving the data. During the event, students were tasked with creating classifiers, while provided only with \gls{sd} \cite{Yale_2020}. They were then scored on the accuracy of their model on real data. In more rigorous initiatives, they attempted (successfully) to recreate the experiments published in medical papers based on the MIMIC dataset using only data generated from their model \gls{healthgan}. In a subsequent version of their article, the authors evaluate the performance of their model against traditional privacy preservation methods by using the trained discriminator component of \gls{healthgan} to discriminate real from synthetic samples.
        
    \subsection{Privacy}
        Some authors offered a privacy risk assessment of their \gls{sd} To evaluate the risk of reidentification, empirical analyses were conducted according to the definitions of \gls{mi}, \gls{ad}  \cite{Choi2017-nt,Goncalves2020,yan2020generating,chen2019ganleaks, chincheong2020generation} and the \gls{rr} \cite{Zhang2020}. Cosine similarities between pairs of samples are also employed \cite{torfi2019generating}. Most studies report low success rates for these types of attacks, and little effect from the sample size, although \citeauthor{chen2019ganleaks} note that sample sizes under 10k lead to higher risk. \par
        
        Some have put forward the notion that preventing over-fitting and preserving privacy may not be conflicting goals \cite{Wu2019-ui,Mukherjee2019-vu}. Numerous attempts have been made to apply traditional privacy guarantees, such as deferentially-private stochastic gradient descent \cite{Beaulieu-Jones2019-ct, esteban2017real,chincheong2020generation, BaeAnomiGAN2020}. By limiting the gradient amplitude at each step and adding random noise, AC-GAN could produce useful data with $\epsilon=3.5$ and $\delta<10^{-5}$ according to the definition of differential privacy. Uniquely, \citeauthor{BaeAnomiGAN2020} ensure privacy with a probabilistic scheme that ensure indistinguishably, but also maximizes utility. Specifically, a multiplicative perturbation by random orthogonal matrices with input entries of $k x m$ medical records and a second second discriminator in the form of a pretrained predictor \cite{BaeAnomiGAN2020}. In black-box and white-box type attacks, including the LOGAN \cite{hayes2017logan} method, \gls{medgan} performed considerably better than \gls{wgan-gp} \cite{chen2019ganleaks}, the algorithm which served as basis for improvements to \gls{medgan} in publications discussed in Section \ref{subsubsec:categorical}. Overall, the author notes that releasing the full model poses a high risk of privacy breaches and that smaller training sets (under 10k) also lead to a higher risk. \citeauthor{Goncalves2020} evaluated \gls{mc-medgan} against multiple non-adversarial generative models in a variety of privacy compromising attacks, including \gls{ad}, obtaining inconsistent results for \gls{mc-medgan} \cite{Goncalves2020}. While this is not mentioned by the authors, multiple results reported in the publication point to the fact that the \gls{gan} was not properly trained or suffered mode-collapse.\par
        
        Means to confer privacy guarantees on \gls{sd} generated by \gls{gan} are being actively researched in a variety of fields, many of which are a priori readily applicable to health data. At this stage, however, contradictory results have between obtained where the statistical fidelity of the synthetic seemed to be preserved, but utility-based measures based on a classification were degraded by incorporating DP. S In privGAN, Mukherjee et al., an adversary is introduced, forcing the generator to produce samples that minimize the risk of MIA attack, in addition to cheating the discriminator. The combination of both goals has the explicit effect of preventing over-fitting, and their algorithm produces samples of similar quality to non-private \gls{gan}.\par
    
        \subsubsection{The status of fully synthetic data in regards to current privacy regulations}
        
            It seems intuitively possible that the artificial nature of \gls{sd} essentially prevents associations with real patients, however the question is never directly addressed in the publications. An extensive Stanford Technological Review legal analysis of \gls{sd} concluded that laws and regulations should not treat \gls{sd} indiscriminately from traditional privacy preservation methods \cite{bellovin2019privacy}. They state that current privacy statutes either outweigh or downplay the potential for \gls{sd} to leak secrets by implicitly including it as the equivalent of anonymization. 
    
        \subsubsection{Alternative views of privacy}
        
            The discordance between the theoretical concepts of DP, which are  based ultimately on infinite samples, and the often insufficient data on which the probability of disclosure is calculated remains deficient. Therefore, Yoon et al. have postulated an intriguing alternative view of privacy \cite{Yoon2020-anon}. They propose to emphasize measuring identifiability of finite patient data, rather than the probabilistic disclosure loss of DP based on unrealistic premises. Simplistically, they define identifiability as the minimum closest distance between any pair of synthetic and real samples. In their implementation, the generator receives both the usual random seed and a real sample as input. This has the effect of mitigating mode collapse, but also of reproducing the real samples. On the other hand, the discriminator is equipped with an additional loss metric based on a measure of similarity between the original sample and the generated one, thus ensuring the tuneable threshold of identifiability is met. Their results on a number of previously discussed evaluation metrics are encouraging.\par
            
            In a similar approach, \citeauthor{Yale_2020} broke away from the theoretical guarantees of traditional methods with a measure native to \gls{gan}. Their proposal is a metric quantifying the loss of privacy, a concept more aligned with the objective of \gls{gan} to minimize the loss of data utility \cite{yale:hal-02160496,p2019}. They point out, quite appropriately, the advantage of concrete measurable values of loss in utility and privacy when making the decision of releasing sensitive data. Briefly, the Nearest Neighbor Adversarial Accuracy measures the loss in privacy based on the difference between two nearest neighbor metrics. The  first component is the proportion of synthetic samples that are closer to any real sample than any pair of real samples. The second component is the reverse operation. In a subsequent paper, \gls{healthgan} evaluated against traditional privacy preservation methods with a variant of the IA based on the nearest neighbor metric. \gls{healthgan} performs considerably better than all other methods, while still maintaining utility on a prediction task.


       



