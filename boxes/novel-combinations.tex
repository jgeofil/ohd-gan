\footnotesize
\tcbset{enhanced,before skip=1cm, nobeforeafter, width=0.5\linewidth}
\begin{tcolorbox}[arc=0mm, 
    colback=cadmiumgreen!10!white, 
    coltext=cadmiumgreen!90!black,  
    colframe=cadmiumgreen!90!black,
    colbacktitle=cadmiumgreen!80,
    leftrule=3mm,
    rightrule=0mm, 
    toprule=0mm, 
    bottomrule=0mm, 
    box align=top]

Unexpected combinations of existing algorithms can harness the in strengths of both, or compensate for lacking, producing performance above the capabilities of both. Evolutionary algorithms are only one particular example, but in fact we've seen a few in this review, including the first that incorporated an \gls{ae} or the techniques borrowed from the \gls{sdv}. Mix-and-match, select, repeat is the principle behind any \gls{ml} model, the notion of meme, and broadly human knowledge... and our existence.
\end{tcolorbox}
\hfill
\begin{tcolorbox}[tcbox width=auto, 
    arc=0mm, 
    colback=white, 
    coltext=cadmiumgreen, 
    boxrule=0pt, 
    colframe=white,
    box align=top]
    
\epigraph{To me, it is very striking to now understand that their work, described in "ImageNet Classification with deep convolutional neural networks", is the combination of very old concepts (a CNN with pooling and convolution layers, variations on the input data) with several new key insight (very efficient GPU implementation, ReLU neurons, dropout), and that this, precisely this, is what modern deep learning is.}{\textit{Andrey Kurenkov \cite{kurenkov2020briefhistory}}}

\end{tcolorbox}
\normalsize